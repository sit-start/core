# 80.0% val rec: https://wandb.ai/kevdale/swin_v2_t_ham10k/runs/9f62d_00006
defaults:
  - _defaults_
  - trial/data: ham10k
  - trial/model: swin_v2_t
  - _self_
eval:
  test:
    metrics:
      rec:
        _target_: sitstart.ml.metrics.AverageMulticlassRecall
        num_classes: 7 # ${trial.data.num_classes}
  train:
    # don't track train split metrics since we're using cutmix/mixup
    metrics: null
  select:
    metric: val_rec
    mode: max
max_num_epochs: 50
param_space:
  train_loop_config:
    loss_fn: { _target_: ray.tune.grid_search, values: [focal] } # note
    gamma: { _target_: ray.tune.grid_search, values: [1.5] }
    weighted_loss: { _target_: ray.tune.grid_search, values: [1] }
    rebalance: { _target_: ray.tune.grid_search, values: [1] }
    lr: { _target_: ray.tune.grid_search, values: [3e-5] }
    dedupe: { _target_: ray.tune.grid_search, values: [1] }
    contrast: { _target_: ray.tune.grid_search, values: [1.0] }
    test_as_val: { _target_: ray.tune.grid_search, values: [0] }
    augment: { _target_: ray.tune.grid_search, values: [1] }
    attention_dropout: { _target_: ray.tune.grid_search, values: [0.0] }
    dropout: { _target_: ray.tune.grid_search, values: [0.0] }
    train_feat: { _target_: ray.tune.grid_search, values: ["567"] }
trial:
  data:
    batch_size: 12
    module:
      rebalance: ${param_space.train_loop_config.rebalance}
      dedupe: ${param_space.train_loop_config.dedupe}
      augment: ${if:${param_space.train_loop_config.augment},${trial.data.augment.transform},null}
  loss_fn:
    _target_: sitstart.ml.losses.FocalLoss
    weight: ${if:${param_space.train_loop_config.weighted_loss},${trial.data.nml_inv_class_freq},null}
    gamma: ${param_space.train_loop_config.gamma}
  model:
    attention_dropout: ${param_space.train_loop_config.attention_dropout}
    dropout: ${param_space.train_loop_config.dropout}
    target_size:
      crop: 443
      resize: 450
    transforms:
      contrast:
        factor: ${param_space.train_loop_config.contrast}
    module:
      require_grad:
        - "-"
        - features.[${param_space.train_loop_config.train_feat}]
        - norm
        - head
  optimizer:
    _partial_: true
    _target_: torch.optim.AdamW
    lr: ${param_space.train_loop_config.lr}
tune:
  scheduler:
    grace_period: ${max_num_epochs}
