checkpoint:
  _target_: ray.train.CheckpointConfig
  checkpoint_score_attribute: ${eval.select.metric}
  checkpoint_score_order: ${eval.select.mode}
  num_to_keep: 2
debug: false
float32_matmul_precision: high
eval:
  test:
    metrics:
      acc:
        _target_: torcheval.metrics.MulticlassAccuracy
  train: ${eval.test}
  select:
    metric: val_loss
    mode: min
gradient_clip:
  value: null
  algorithm: null
hydra:
  run:
    dir: ${oc.env:HOME}/.local/share/hydra/outputs/${hydra.job.name}/${now:%Y-%m-%d_%H-%M-%S}
max_num_epochs: 1
name: ${hydra:job.config_name}
num_sanity_val_steps: null
logging_interval: 100
param_space:
  _convert_: all
  scaling_config:
    _target_: ray.train.ScalingConfig
    num_workers: 1
    resources_per_worker:
      GPU: 1
    use_gpu: ${gt:${.resources_per_worker.GPU},0}
  train_loop_config: {}
restore:
  checkpoint_path: null
  run:
    group: null
    trial_id: null
    select: last # best or last
save_repo_state: true
seed: null
storage_path: s3://sitstart-ray-runs
torch:
  distributed_backend: nccl
trial:
  data:
    augment:
      transforms: {}
      transform:
        _convert_: all
        _target_: torchvision.transforms.Compose
        transforms: ${oc.dict.values:..transforms}
    dims: null
    batch_size: ???
    num_classes: ???
    module:
      _partial_: true
      _target_: ???
      augment: ${trial.data.augment.transform}
      batch_size: ${..batch_size}
      transform: ${trial.model.transform}
  loss_fn:
    _target_: torch.nn.CrossEntropyLoss
  lr_scheduler: null
  model:
    dims: null
    module:
      _target_: ???
    target_size:
      crop: null
      resize: null
    transforms: {}
    transform:
      _convert_: all
      _target_: torchvision.transforms.Compose
      transforms: ${oc.dict.values:..transforms}
  optimizer:
    _partial_: true
    _target_: torch.optim.SGD
    lr: ???
  training_module:
    _partial_: true
    _target_: sitstart.ml.training_module.TrainingModule
tune:
  long_trial_names: true
  num_samples: 1
  scheduler:
    _target_: ray.tune.schedulers.ASHAScheduler
    grace_period: ${max:1,${floordiv:${.max_t},5}}
    max_t: ${max_num_epochs}
    metric: ${eval.select.metric}
    mode: ${eval.select.mode}
    reduction_factor: 2
  search_alg: random
wandb:
  enabled: true
