cluster_name: g5g
upscaling_speed: 1.0
idle_timeout_minutes: 10
provider:
  type: aws
  region: us-west-2
  availability_zone: us-west-2a,us-west-2b,us-west-2c
auth:
  ssh_user: ec2-user
  ssh_private_key: ~/.ssh/rsa.pem
available_node_types:
  m6a.xlarge:
    max_workers: 0
    resources: {}
    node_config:
      InstanceType: m6a.xlarge
      ImageId: ami-0c38021ff804d9342
      KeyName: rsa
      IamInstanceProfile:
        Arn: arn:aws:iam::960487471244:instance-profile/ray-head
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: 256
            VolumeType: gp3
  g5.12xlarge:
    min_workers: 0
    max_workers: 1
    resources: { "CPU": 48, "GPU": 4 }
    node_config:
      InstanceType: g5.12xlarge
      ImageId: ami-0c38021ff804d9342
      KeyName: rsa
      IamInstanceProfile:
        Arn: arn:aws:iam::960487471244:instance-profile/ray-worker
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: 256
            VolumeType: gp3
  g5.xlarge:
    min_workers: 0
    # quota allows 8, but max_workers=4 allows 4 xlarge and 1 12xlarge
    max_workers: 4
    resources: { "CPU": 4, "GPU": 1 }
    node_config:
      InstanceType: g5.xlarge
      ImageId: ami-0c38021ff804d9342
      KeyName: rsa
      IamInstanceProfile:
        Arn: arn:aws:iam::960487471244:instance-profile/ray-worker
      BlockDeviceMappings:
        - DeviceName: /dev/xvda
          Ebs:
            VolumeSize: 256
            VolumeType: gp3
head_node_type: m6a.xlarge
file_mounts: {
    # contents of the dev repo
    "~/dev": "~/dev",
    # relevant contents of the dotfiles repo
    "~/.aws/config": "~/.aws/config",
    "~/.local/bin/scm-prompt.sh": "~/.local/bin/scm-prompt.sh",
    "~/.bash_profile": "~/.bash_profile",
    "~/.bashrc": "~/.bashrc",
    "~/.ipython/profile_default/ipython_config.py": "~/.ipython/profile_default/ipython_config.py",
    "~/.ipython/profile_default/ipython_kernel_config.py": "~/.ipython/profile_default/ipython_kernel_config.py",
    "~/.vscode-server/data/Machine/settings.json": "~/.vscode-server/data/Machine/settings.json",
  }
cluster_synced_files: []
file_mounts_sync_continuously: True
rsync_exclude: []
rsync_filter:
  - ".gitignore"
initialization_commands: []
setup_commands:
  # fetch repo deploy keys
  - |
    repos=(dev core notebooks dotfiles)
    for repo in "${repos[@]}"; do
      key_name="${repo}_deploy_ed25519"
      test -f ~/.ssh/"$key_name" ||
        (aws secretsmanager get-secret-value --secret-id "$key_name" \
          --query SecretString --output text |
          jq --raw-output .private >~/.ssh/"$key_name" &&
          chmod go-rwx ~/.ssh/"$key_name")
    done
  # add github keys to known hosts
  - |
    github_keys=$(curl -s https://api.github.com/meta | jq -r .ssh_keys[])
    grep -xqF "$github_keys" ~/.ssh/known_hosts 2>/dev/null ||
      while IFS= read -r key; do
        echo "github.com $key" >>~/.ssh/known_hosts
      done <<<"$github_keys"
  # setup a deploy key for read-only access to the core repository
  # TODO: support read-only access for all repos via custom clone
  - |
    grep -xqF "Host github.com" ~/.ssh/config 2>/dev/null ||
      echo -e "\nHost github.com\n  IdentityFile ~/.ssh/core_deploy_ed25519" \
        >>~/.ssh/config && chmod go-rwx ~/.ssh/config
  # fetch/prune so local tags on nodes don't conflict with new remote tags
  - git -C $DEV/core fetch --all --prune --prune-tags
  # login and configure wandb
  - mkdir -p $WANDB_DIR
  - wandb login
  # install new components not in the AMI (TODO: update AMI)
  - |
    gh >/dev/null 2>&1 ||
      sudo bash -c "source $DEV/core/scripts/ec2_dev_setup.sh &&
      install_components github"
head_setup_commands:
  # create the prometheus service file
  - |
    sudo cp "$DEV/core/python/ktd/ray/config/aux/prometheus.service" \
      /etc/systemd/system/prometheus.service
  # update the fluent-bit config
  - |
    sudo mkdir -p /etc/fluent-bit &&
      sudo cp "$DEV/core/python/ktd/ray/config/aux/fluent-bit-head.conf" \
        /etc/fluent-bit/fluent-bit.conf
  # update the grafana service file to ensure grafana can acccess Ray
  # config files in /tmp/ray
  - |
    sudo sed -i 's/^PrivateTmp=true/PrivateTmp=false/g' \
      /usr/lib/systemd/system/grafana-server.service
  # update the grafana service config
  - |
    sudo test -f /etc/grafana/grafana.ini.ray-bak || (
      sudo cp /etc/grafana/grafana.ini /etc/grafana/grafana.ini.ray-bak &&
        sudo cp "$DEV/core/python/ktd/ray/config/aux/grafana.ini" \
          /etc/grafana/grafana.ini)
  # install loki TODO: move to AMI
  - |
    yum list installed loki >/dev/null 2>&1 || 
      sudo -- bash -c 'source ~ec2-user/dev/core/scripts/ec2_dev_setup.sh; \
        install_components loki'
worker_setup_commands: []
head_start_ray_commands:
  # restart ray
  - ray stop
  - |
    ray start --head --port=6379 --object-manager-port=8076 \
      --autoscaling-config=~/ray_bootstrap_config.yaml \
      --dashboard-host=0.0.0.0 --ray-debugger-external
  # add loki dashboard and datasource to grafana
  - |
    cp "$DEV/core/python/ktd/ray/config/aux/loki_grafana_datasource.yml" \
      /tmp/ray/session_latest/metrics/grafana/provisioning/datasources
    cp "$DEV/core/python/ktd/ray/config/aux/loki_grafana_dashboard.json" \
      /tmp/ray/session_latest/metrics/grafana/dashboards/

  # restart services
  - sudo systemctl restart grafana-server
  - sudo systemctl restart prometheus
  - sudo systemctl restart fluent-bit
  - sudo systemctl restart loki
worker_start_ray_commands:
  # restart ray
  - ray stop
  - |
    ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076 \
      --ray-debugger-external
  # update the fluent-bit config with $RAY_HEAD_IP
  - |
    sudo mkdir -p /etc/fluent-bit &&
      cat "$DEV/core/python/ktd/ray/config/aux/fluent-bit-worker.conf" |
        envsubst | sudo tee /etc/fluent-bit/fluent-bit.conf
  # restart the fluent-bit service
  - sudo systemctl restart fluent-bit
